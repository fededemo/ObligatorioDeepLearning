{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OFm1E4ALuev"
   },
   "source": [
    "# Obligatorio de Deep Learning: Detección de actividad anormal en logs de HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPryBvEdLapR"
   },
   "source": [
    "Federico De León  \n",
    "Daniel Dominitz  \n",
    "Juan Emilio Gabito (138616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoMkgREvLue_"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GewshNtLufB"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-yEMLaWNgaf",
    "outputId": "06deae33-146d-4788-b578-5b0439d7e88e"
   },
   "outputs": [],
   "source": [
    "# Si estamos en Colab setemos Drive como FS\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiiNR7s1L08y"
   },
   "outputs": [],
   "source": [
    "# Para Colab nos paramos en alguna de las carpetas del grupo\n",
    "import os\n",
    "if os.path.exists(\"/content/drive/MyDrive/ORT/DeepLearning/Obligatorio\"):\n",
    "  os.chdir('/content/drive/MyDrive/ORT/DeepLearning/Obligatorio')\n",
    "else:\n",
    "  if os.path.exists(\"/content/drive/MyDrive/ORT/Posgrado AI/Taller DeepLearning/Obligatorio\"):\n",
    "    os.chdir(\"/content/drive/MyDrive/ORT/Posgrado AI/Taller DeepLearning/Obligatorio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aasM3UopLufC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Embedding, LSTM, Dense, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "utils = reload(utils)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU26rNfHLufK"
   },
   "source": [
    "### Reproducibilidad y repetibilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v2lH6nrLapV"
   },
   "source": [
    "Para poder asegurar la reproducibilidad y repetibilidad de los experimentos en distintas ejecuciones seteamos la _seed_ tanto en Numpy como en Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLlbGI8aLufL"
   },
   "outputs": [],
   "source": [
    "np.random.seed(117)\n",
    "tf.random.set_seed(117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezE0iqNALufO"
   },
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9RPPY70LufP"
   },
   "outputs": [],
   "source": [
    "hdfs_train, hdfs_test_kaggle = utils.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "LFTLnExcLufQ",
    "outputId": "27a1bc2a-1d4e-4db5-b02f-fa38f22a0070",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdfs_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mbSin2w_LufS",
    "outputId": "d0061a78-b832-4a88-fd93-87f980387e4f"
   },
   "outputs": [],
   "source": [
    "hdfs_test_kaggle[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-nyjbgRLufS"
   },
   "source": [
    "## 3. Análisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA1nSPKuLufT"
   },
   "source": [
    "### Análisis descriptivo general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "lwVUrlooLufU",
    "outputId": "2d8d653e-2fe4-477f-fb17-6e18b66a93bd"
   },
   "outputs": [],
   "source": [
    "hdfs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggf0msF3LapY"
   },
   "outputs": [],
   "source": [
    "LABELS = ['Normal', 'Abnormal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "i4J3n780LufV",
    "outputId": "ac917ad7-0a44-4a2a-ef7b-e2ccf9791e0d"
   },
   "outputs": [],
   "source": [
    "utils.plot_data(hdfs_train, LABELS, \"Datos disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stsh7DznLufW"
   },
   "source": [
    "### Análisis de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxC6UWE9LufW"
   },
   "outputs": [],
   "source": [
    "#Agregar ploteo de largos de secuencias, distribuciones por simbolo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3K7nfmctLufY",
    "outputId": "93818837-23bf-4e0b-c42c-a30837e079d9"
   },
   "outputs": [],
   "source": [
    "raw_sequences, data_y = utils.load_sequences_and_target(hdfs_train, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UADGRwMaYeC",
    "outputId": "1ff0ba5e-c58a-4d0d-bc1e-d83bd3233b2c"
   },
   "outputs": [],
   "source": [
    "data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-Q0p2ffLufa",
    "outputId": "d4cce17a-4541-4cf9-8917-80699a126d16"
   },
   "outputs": [],
   "source": [
    "min([min(s) for s in raw_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxRfgSmULufb",
    "outputId": "e9c4c70f-c4b7-46d0-948f-d45790c6377e"
   },
   "outputs": [],
   "source": [
    "max([max(s) for s in raw_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6fCiLzXLufc",
    "outputId": "1c057506-2f27-4d6b-cf2b-ff8e6db4f79b"
   },
   "outputs": [],
   "source": [
    "np.median([len(s) for s in raw_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TV8BPzVLapa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deberíamos agregar la mean también"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "m0jJwYfvVps3",
    "outputId": "0fc1c847-6d07-418c-8d02-2299eee4288f"
   },
   "outputs": [],
   "source": [
    "# Esto no tengo ni idea qué es. ¿Qué aporta? ¿Es parte del EDA?\n",
    "# get the cdf values of the len\n",
    "y = np.arange(len([len(s) for s in raw_sequences])) / float(len([len(s) for s in raw_sequences]))\n",
    "\n",
    "#fig size\n",
    "plt.figure(figsize=(20,10)) \n",
    "#labels\n",
    "plt.xlabel('Len raw sequences')\n",
    "plt.ylabel('Distribution')\n",
    "#title\n",
    "plt.title('Cumulative Distribution Function of raw sequences')  \n",
    "#plot \n",
    "plt.plot(np.sort([len(s) for s in raw_sequences]), y, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Zao1sfPtbpI"
   },
   "source": [
    "# Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kALO2NbALapb"
   },
   "source": [
    "Previo a realizar el preprocesamiento de los datos y el posterior entrenamiento y evaluación se los modelos setearemos la constante\n",
    "_MAX\\_LEN_ en 19. Este valor se obtuvo del largo promedio de las secuencias y en base a experimentación con\n",
    "tamaños maores y menores se pudo ver que 19 ofrecía los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvgNuWybLapb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgHGgcpXybsv"
   },
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STkk44KtLufh"
   },
   "outputs": [],
   "source": [
    "padded_sequences = utils.pad_sequences(raw_sequences, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvvcyBuXtP5O"
   },
   "source": [
    "## Particionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXOX6eLuLufj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = utils.split(padded_sequences, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "HV_e2FLBa_Hw",
    "outputId": "9250a940-fd2b-4198-e231-0509f036a365"
   },
   "outputs": [],
   "source": [
    "utils.plot_data(y_train, LABELS, \"Datos de entrenamiento por clase\", has_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90rrw_79MsbM"
   },
   "source": [
    "\n",
    "## Generación de datos con Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whcOuiXoMrFC"
   },
   "outputs": [],
   "source": [
    "X_train_for_aug, dummy_a, dummy_b, dummy_c, dummy_d, dummy_e = utils.split(raw_sequences, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la cantidad de ventanas para el data augmentation\n",
    "WINDOWS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcowAy5dM2vV"
   },
   "outputs": [],
   "source": [
    "X_train_aug, y_train_aug = utils.sequences_augmentation(X_train_for_aug, y_train, MAX_LEN, WINDOWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9whfbpEe6cB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "padding to data augmentation train\n",
    "\"\"\"\n",
    "X_train_aug = utils.pad_sequences(X_train_aug, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "IcNAq4O_bjk9",
    "outputId": "76ef02b7-1136-489f-93c3-288c02772223"
   },
   "outputs": [],
   "source": [
    "utils.plot_data(y_train_aug, LABELS, \"Datos de entrenamiento luego de Data Augmentation por clase\", has_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQHGdWmIblVc"
   },
   "source": [
    "# Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5yVDCJ-aT1X"
   },
   "source": [
    "## Modelo Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyfvmijZahXO"
   },
   "outputs": [],
   "source": [
    "vocab_size = max([max(s) for s in raw_sequences]) + 1\n",
    "batch_size = 10\n",
    "epochs = 2\n",
    "patience = 10\n",
    "optimizer = Adam() \n",
    "loss = 'categorical_crossentropy'\n",
    "embedding_size = math.ceil(vocab_size**0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4a10vr0hoEy"
   },
   "source": [
    "### Truncating = 'Pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OLGsOdzaf0R",
    "outputId": "5ec6917c-c17f-4e32-f1a4-2e7613b65dbe"
   },
   "outputs": [],
   "source": [
    "initial_model = Sequential()\n",
    "initial_model.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "initial_model.add(LSTM(64, return_sequences=False))\n",
    "initial_model.add(Dense(2, activation='softmax'))\n",
    "initial_model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "initial_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejiHGPbacrBk"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WioIf9ibcL6g",
    "outputId": "9150d1c5-4e69-4415-de56-bb63a903167a"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "initial_training, initial_model = utils.train(initial_model,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qso0vHEen_u"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVaOvPUreqea"
   },
   "outputs": [],
   "source": [
    "initialmodel_accuracy, initialmodel_precision, initialmodel_recall, initialmodel_f1 = utils.eval_model(initial_training, initial_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBTb_qv_hyEE"
   },
   "source": [
    "### Truncating = 'Post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Pmy8m9Ahz1l"
   },
   "outputs": [],
   "source": [
    "initial_model_post = Sequential()\n",
    "initial_model_post.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "initial_model_post.add(LSTM(64, return_sequences=False))\n",
    "initial_model_post.add(Dense(2, activation='softmax'))\n",
    "initial_model_post.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "initial_model_post.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JoqPhm1jXt6"
   },
   "source": [
    "#### Datos con padding al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VpkyW5wj3mJ"
   },
   "outputs": [],
   "source": [
    "padded_sequences_post = utils.pad_sequences(raw_sequences, MAX_LEN,truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le_uiVo-jc5S"
   },
   "outputs": [],
   "source": [
    "X_train_post, X_test_post, X_val_post, y_train_post, y_test_post, y_val_post = utils.split(padded_sequences_post, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teUEp_I5h5JN"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ouqhJ6mh2l1"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "initial_training_post, initial_model_post = utils.train(initial_model_post,\n",
    "                X_train_post,\n",
    "                y_train_post, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val_post, \n",
    "                validation_data_y = y_val_post,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAFRn7P6h_uG"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNtsd9zth86p"
   },
   "outputs": [],
   "source": [
    "initialmodel_post_accuracy, initialmodel_post_precision, initialmodel_post_recall, initialmodel_post_f1 = utils.eval_model(initial_training_post, initial_model_post, X_test_post, y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSCHOiOpn1DK"
   },
   "source": [
    "### Comentarios padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnp3_OEGraha"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3%}'.format\n",
    "data = [['Initial Model Pre Truncating',initialmodel_accuracy, initialmodel_precision, initialmodel_recall, initialmodel_f1], ['Initial Model Post Truncating',initialmodel_post_accuracy, initialmodel_post_precision, initialmodel_post_recall, initialmodel_post_f1]]\n",
    "pd.DataFrame(data, columns=[\"Modelo\", \"Accuracy\", \"Precision\",\"Recall\",\"F1-score\"]).sort_values(by='F1-score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQCtQPD4aaLg"
   },
   "source": [
    "## Modelo Mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09FPLKOcac5X"
   },
   "outputs": [],
   "source": [
    "vocab_size = max([max(s) for s in raw_sequences]) + 1\n",
    "batch_size = 24\n",
    "epochs = 7\n",
    "patience = 10\n",
    "optimizer = Adam() \n",
    "loss = 'categorical_crossentropy'\n",
    "embedding_size = math.ceil(vocab_size**0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NqYUvmaa9ib"
   },
   "outputs": [],
   "source": [
    "improved_model = Sequential()\n",
    "improved_model.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "improved_model.add(LSTM(64, return_sequences=True))\n",
    "improved_model.add(LSTM(64, return_sequences=False))\n",
    "improved_model.add(Dense(2, activation='softmax'))\n",
    "improved_model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0My0t2dgpwC"
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B08Hin7cQj4"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "improved_training, improved_model = utils.train(improved_model,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "986p6T3zeeBP"
   },
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6Lrj1DGebOd"
   },
   "outputs": [],
   "source": [
    "improved_model_accuracy, improved_model_precision, improved_model_recall, improved_model_f1 = utils.eval_model(improved_training, improved_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmmLNAeGLufi"
   },
   "source": [
    "## Grid Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19ak24O67mgB"
   },
   "outputs": [],
   "source": [
    "vocab_size = 30\n",
    "patience = 5\n",
    "embedding_size=3\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"epochs\": [2, 8, 15],\n",
    "    \"units\": [2,64, 72],\n",
    "    \"vocab_size\": [vocab_size + 1,],\n",
    "    \"embedding_size\": [embedding_size],\n",
    "    \"max_len\": [MAX_LEN,],\n",
    "    \"optimizer\": [\"adam\",],\n",
    "    \"loss\": [\"categorical_crossentropy\",]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0AXZGdMpTLh",
    "outputId": "368d8a11-e154-4da2-e751-96330124f7c5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entrenamiento\n",
    "\"\"\"\n",
    "gs = utils.grid_search(params, utils.build_improved_model, 3)\n",
    "grid_result = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8rsOI8OTyCg",
    "outputId": "7928440b-c87d-40f0-e054-d08b71d5033c"
   },
   "outputs": [],
   "source": [
    "grid_result.best_params_ #comentar que tambien se puede obtener el best model que en definitiva es lo mismo que hacemos en Modelo Mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt8xXza778Xr"
   },
   "outputs": [],
   "source": [
    "batch_size = grid_result.best_params_['vocab_size']\n",
    "epochs = grid_result.best_params_['epochs']\n",
    "units = grid_result.best_params_['units']\n",
    "optimizer = Adam() \n",
    "loss = grid_result.best_params_['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADJO-d2CvESK"
   },
   "source": [
    "### Model Mejorado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRVizv75P9BC"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "model.add(LSTM(units, return_sequences=True))\n",
    "model.add(LSTM(units, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zve-6Z7pcios"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_S-tD8EyzZI"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "training, model = utils.train(model,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8jMNxJ3zehB"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9gXg_A5zgrp"
   },
   "outputs": [],
   "source": [
    "model_accuracy, model_precision, model_recall, model_f1 = utils.eval_model(training, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku-bAnBHyy3C"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHEwCal3yfWk"
   },
   "outputs": [],
   "source": [
    "name='model'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "#generate kaggle\n",
    "utils.load_test_sequences_and_generate_prediction_file(model, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0-VGKtjvmZy"
   },
   "source": [
    "### Modelo Mejorado con Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkvaa5sRwL0p"
   },
   "outputs": [],
   "source": [
    "modelBN = Sequential()\n",
    "modelBN.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "modelBN.add(LSTM(units, return_sequences=True))\n",
    "modelBN.add(LSTM(units, return_sequences=False))\n",
    "modelBN.add(BatchNormalization())\n",
    "modelBN.add(Dense(2, activation='softmax'))\n",
    "modelBN.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelBN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVsoksYzy8GL"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGXGITxyy-PA"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "trainingBN, modelBN = utils.train(modelBN,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJOugunezMBL"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNpnWLAEzPHN"
   },
   "outputs": [],
   "source": [
    "modelBN_accuracy, modelBN_precision, modelBN_recall, modelBN_f1 = utils.eval_model(trainingBN, modelBN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaySRDssyxdo"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9ZdvKvyyaOg"
   },
   "outputs": [],
   "source": [
    "name='ModelWithBN'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "#generate kaggle\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelBN, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelBN.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelBN.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMDITbd1th35"
   },
   "source": [
    "### Modelo Mejorado con Gradient Clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acS1nYp7tj_4"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(clipvalue=0.5) #para agregar clip value clipvalue=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsuT-Q6Luf-0"
   },
   "outputs": [],
   "source": [
    "modelGC = Sequential()\n",
    "modelGC.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "modelGC.add(LSTM(units, return_sequences=True))\n",
    "modelGC.add(LSTM(units, return_sequences=False))\n",
    "modelGC.add(Dense(2, activation='softmax'))\n",
    "modelGC.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelGC.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDV9Xkn9uJ-F"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES5o-WICuTT-"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "trainingGC, modelGC = utils.train(modelGC,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T7ut3fwuL4C"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c6Anzblvqku"
   },
   "outputs": [],
   "source": [
    "modelGC_accuracy, modelGC_precision, modelGC_recall, modelGC_f1 = utils.eval_model(trainingGC, modelGC, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0Z5ogu9uO_H"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezIHobguvx6R"
   },
   "outputs": [],
   "source": [
    "name='ModelWithGC'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "#generate kaggle\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelGC, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelGC.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelGC.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KExkov0evvyO"
   },
   "source": [
    "### Modelo Mejorado con Gradient *Normalization*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BZB6uzNvzg2"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(clipnorm=1.) #para agregar clip value clipvalue=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox-Qar9UwLBu"
   },
   "outputs": [],
   "source": [
    "modelGN = Sequential()\n",
    "modelGN.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "modelGN.add(LSTM(units, return_sequences=True))\n",
    "modelGN.add(LSTM(units, return_sequences=False))\n",
    "modelGN.add(Dense(2, activation='softmax'))\n",
    "modelGN.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelGN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT1YJ4SMy_99"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pGKzdnnzFTr"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "trainingGN, modelGN = utils.train(modelGN,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaK5JqSAzUfx"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnZO4elgzZQo"
   },
   "outputs": [],
   "source": [
    "modelGN_accuracy, modelGN_precision, modelGN_recall, modelGN_f1 = utils.eval_model(trainingGN, modelGN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRxM9wbpywCg"
   },
   "source": [
    "####Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGHxtm-nx5dr"
   },
   "outputs": [],
   "source": [
    "name='ModelWithGN'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "#generate kaggle\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelGN, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelGN.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelGN.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF-SekK0vQJM"
   },
   "source": [
    "### Modelo Mejorado con Batch Normalization y Gradient Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CqjJVXdLufm"
   },
   "outputs": [],
   "source": [
    "modelGN_BN = Sequential()\n",
    "modelGN_BN.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "modelGN_BN.add(LSTM(units, return_sequences=True))\n",
    "modelGN_BN.add(LSTM(units, return_sequences=False))\n",
    "modelGN_BN.add(BatchNormalization())\n",
    "modelGN_BN.add(Dense(2, activation='softmax'))\n",
    "modelGN_BN.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelGN_BN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weaHMbBAzoqD"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoKoxraqzpFy"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "trainingGN_BN, modelGN_BN = utils.train(modelGN_BN,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYaONMdvzWUp"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ0bbNNzzXFP"
   },
   "outputs": [],
   "source": [
    "modelGN_BN_accuracy, modelGN_BN_precision, modelGN_BN_recall, modelGN_BN_f1 = utils.eval_model(trainingGN_BN, modelGN_BN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cDTnLsaxjyE"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QslTNmcixobB"
   },
   "outputs": [],
   "source": [
    "name='ModelWithGNandBN'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelGN_BN, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelGN_BN.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelGN_BN.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abIJFm--rMwZ"
   },
   "source": [
    "### Modelo Mejorado con Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zp7UIMUroGg"
   },
   "outputs": [],
   "source": [
    "modelDA = Sequential()\n",
    "modelDA.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN))\n",
    "modelDA.add(LSTM(units, return_sequences=True))\n",
    "modelDA.add(LSTM(units, return_sequences=False))\n",
    "modelDA.add(Dense(2, activation='softmax'))\n",
    "modelDA.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelDA.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gPfHVc5rVYF"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpvnDb2rrzRf"
   },
   "outputs": [],
   "source": [
    "#se agregó class.weights\n",
    "trainingDA, modelDA = utils.train(modelDA,\n",
    "                X_train_aug,\n",
    "                y_train_aug, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-YIJY2rX-a"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75noSDBfru30"
   },
   "outputs": [],
   "source": [
    "modelDA_accuracy, modelDA_precision, modelDA_recall, modelDA_f1 = utils.eval_model(trainingDA, modelDA, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F357yN7Ircou"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-cutHTisgMh"
   },
   "outputs": [],
   "source": [
    "name='ModelWithDataAugmentation'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelDA, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelDA.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelDA.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x4jGC-craoZ"
   },
   "source": [
    "### Modelo Mejorado con Data Augmentation y Gradient Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1V5Kv-Tnxc1"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(clipnorm=1.) #para agregar clip value clipvalue=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2jG01Mcn3HL"
   },
   "outputs": [],
   "source": [
    "modelGN_DA = Sequential()\n",
    "modelGN_DA.add(Embedding(vocab_size+1, embedding_size, input_length=MAX_LEN)) \n",
    "modelGN_DA.add(LSTM(units, return_sequences=True))\n",
    "modelGN_DA.add(LSTM(units, return_sequences=False))\n",
    "modelGN_DA.add(Dense(2, activation='softmax'))\n",
    "modelGN_DA.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "modelGN_DA.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oD1ArUZoCDb"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvzkU-tFoECM"
   },
   "outputs": [],
   "source": [
    "trainingGN_DA, modelGN_DA = utils.train(modelGN_DA,\n",
    "                X_train_aug,\n",
    "                y_train_aug, \n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data_X = X_val, \n",
    "                validation_data_y = y_val,                                \n",
    "                patience = patience,\n",
    "                class_weights = utils.class_weights(hdfs_train,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGebLAdToPej"
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCSqSovKoWNm"
   },
   "outputs": [],
   "source": [
    "modelGN_DA_accuracy, modelGN_DA_precision, modelGN_DA_recall, modelGN_DA_f1 = utils.eval_model(trainingGN_DA, modelGN_DA, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxwK6gM0oQyY"
   },
   "source": [
    "#### Generación modelo y Salida Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p23PKPKUoUVe"
   },
   "outputs": [],
   "source": [
    "name='ModelWithDataAugmentationAndGradientNormalization'\n",
    "\n",
    "\"\"\"\n",
    "Generate data Kaggle\n",
    "\"\"\"\n",
    "\n",
    "utils.load_test_sequences_and_generate_prediction_file(modelGN_DA, hdfs_test_kaggle, MAX_LEN, name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs) )\n",
    "\n",
    "\"\"\"\n",
    "Generate Model\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = modelGN_DA.to_json()\n",
    "with open(\"outputs/model_\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelGN_DA.save_weights(\"outputs/model_Weigths\"+name+'_maxLen'+str(MAX_LEN)+'_batchSize'+str(batch_size)+'_epochs'+str(epochs)+'_'+dt.datetime.today().strftime('%Y%m%d_%H%M%S')+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lXsSd3MLugC"
   },
   "source": [
    "## Conclusiones y reflexiones finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LDaD0UnLapt"
   },
   "source": [
    "## Bibliogafìía y referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gevnjEmGLugD"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3%}'.format\n",
    "data = [['Initial Model',initialmodel_accuracy, initialmodel_precision, initialmodel_recall, initialmodel_f1], ['Improved Model',improved_model_accuracy, improved_model_precision, improved_model_recall, improved_model_f1], ['Improved Model Grid Search',model_accuracy, model_precision, model_recall, model_f1], ['Improved Model Grid Search with BN',modelBN_accuracy, modelBN_precision, modelBN_recall, modelBN_f1], ['Improved Model Grid Search with GC',modelGC_accuracy, modelGC_precision, modelGC_recall, modelGC_f1], ['Improved Model Grid Search with GN',modelGN_accuracy, modelGN_precision, modelGN_recall, modelGN_f1], ['Improved Model Grid Search with BN & GN',modelGN_BN_accuracy, modelGN_BN_precision, modelGN_BN_recall, modelGN_BN_f1], ['Improved Model Grid Search with DA',modelDA_accuracy, modelDA_precision, modelDA_recall, modelDA_f1], ['Improved Model Grid Search with DA & GN',modelGN_DA_accuracy, modelGN_DA_precision, modelGN_DA_recall, modelGN_DA_f1]]\n",
    "pd.DataFrame(data, columns=[\"Modelo\", \"Accuracy\", \"Precision\",\"Recall\",\"F1-score\"]).sort_values(by='F1-score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cA1nSPKuLufT",
    "stsh7DznLufW"
   ],
   "provenance": []
  },
  "interpreter": {
   "hash": "c12d86b63e74ac4f427027e3d58b1fa461278ba669d7be39fd5f25ce787fdf43"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
